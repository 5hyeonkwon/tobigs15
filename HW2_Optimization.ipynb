{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74340391, 0.07097286, 0.4486071 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{1+e^{-z}}$ , 단 $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters)\n",
    "    p = 1/(1 + np.exp(-z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5786357960232279"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) =-\\Sigma\\{y_i\\log(p) + (1-y_i)\\log(1-p)\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X,parameters)\n",
    "    loss = y*np.log(p) + (1-y)*np.log(1-p)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X,parameters)\n",
    "    loss = 0.5 * (y - y_hat)^2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X,y, parameters)\n",
    "    loss = loss / n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.120295402874429"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=-\\Sigma(y_i - \\theta^{T}X_i)x_{ij}$\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i - p_i)x_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = (y - y_hat)*X[j]\n",
    "    else:\n",
    "        p = logistic(X,parameters)\n",
    "        gradient = (y - p)*X[j]\n",
    "    return -gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08254939233985932"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X,y,parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58.249345383345485, -5.517014677446889, 29.588753261787282]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "### 설명: 이 함수는 학습시 어떠한 optimizer를 사용할지 지정하는 함수 이다.  batch_size가 1일 경우 모든 데이터를 하나씩 return하기 때문에 Stochastic Gradient Descent에 해당하고 1 < batch_size < len(X_train) 인 경우는 데이터를 batch 단위로 묶어서 최적화를 진행하기 때문에 Mini-batch Gradient Descent에 해당하고 batch_size = len(X_train)의 경우 전체 데이터를 한번에 학습을 하고 갱신하기 때문에 Batch Gradient Descent에 해당한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16091046, 0.126143  , 0.15271956])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: batch 단위가 끝나면 1 epoch 학습했다고 함\n",
    "- num_epoch: 최대 몇번 반복하여 학습을 할 것인지\n",
    "\n",
    "<br>\n",
    "\n",
    "BGD: 데이터의 gradients를 구하고 평균을 진행한 후 한꺼번에 갱신 한다. 즉, 데이터 전체를 다 보고 갱신이 일어나므로 학습이 굉장히 오래 걸리지만 성능은 좋다. 또한 정확한 방향으로 수렴한다. 하지만 local minimum에 빠질 위험히 크다. <br>\n",
    "SGD: 임의의 데이터 하나의 gradients를 구하고 즉시 갱신한다. BGD에 비해 시간이 빠르지만 임의의 데이터를 추출하기 때문에 진자 운동과 같은 결과로 이어져 수렴이 늦을 수 있다. <br>\n",
    "MGD: BGD와 SGD의 단점을 보완하기 위해 데이터를 작은 집단으로 나누어 한꺼번에 gradients를 구하고 갱신을 진행한다. \n",
    "<br>\n",
    "\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> \"Stochastic Gradient Descent\"  \n",
    "batch_size=k -> \"Mini batch Gradient Descent\"  \n",
    "batch_size=whole -> \"Batch Gradient Descent\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch,parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(idx))\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters,loss_function,len(idx))\n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.34313555823009295  params: [-0.89039903  0.88120829 -1.09472595]  gradients: [0.09143608130040028, 0.09783991681556575, 0.15418267365622232]\n",
      "epoch: 100  loss: 0.15040226317802166  params: [-1.95228052  4.17931809 -4.05832419]  gradients: [0.05011304952283638, 0.07506084589472427, 0.0853821789698796]\n",
      "epoch: 200  loss: 0.15039902311686212  params: [-1.95231597  4.17942224 -4.05841886]  gradients: [0.05011266775622351, 0.07506045689254029, 0.08538107396980038]\n",
      "epoch: 300  loss: 0.1503990228910306  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.05011266772961534, 0.07506045686542741, 0.08538107389278259]\n",
      "epoch: 400  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n",
      "epoch: 500  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n",
      "epoch: 600  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n",
      "epoch: 700  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n",
      "epoch: 800  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n",
      "epoch: 900  loss: 0.15039902289101492  params: [-1.95231597  4.17942225 -4.05841887]  gradients: [0.050112667729613404, 0.0750604568654254, 0.08538107389277716]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.95231597,  4.17942225, -4.05841887])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train,)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    default값으로 학습을 시킨 결과이다. 이 경우 밑의 경우와 다르게 learning_rate 가 0.1 인데 loss를 확인해 보면 learning_rate = 0.01 보다 빠르게 줄어드는것을 확인할 수 있다.\n",
    "    \n",
    "    learning_rate가 0.1 일때의 loss는 300 epochs 이후로는 더이상 변하지 않으며 parameters 또한 변하지 않는다는것을 확인할 수 있지만 learning_rate가 0.01 일때는 꾸준히 loss가 떨어지며 parameters 또한 조금씩 변하고 있다는것을 확인할 수 있다. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8592137441374152  params: [-0.15293751  0.74969845  0.27523854]  gradients: [0.05056333323546503, 0.049499361335893, 0.10582634514499047]\n",
      "epoch: 100  loss: 0.3061007800054585  params: [-1.63894158  3.48547137 -3.31789765]  gradients: [0.010972094477515576, 0.0027771731623775255, 0.003250675596712339]\n",
      "epoch: 200  loss: 0.3003347990452867  params: [-1.79438357  3.99031451 -3.78640669]  gradients: [0.013335375999857763, 0.003446464685886267, 0.0007817383081805951]\n",
      "epoch: 300  loss: 0.29946359435013864  params: [-1.84593016  4.15659675 -3.93969121]  gradients: [0.01407116688391399, 0.0036492920963335095, 7.348220561099066e-05]\n",
      "epoch: 400  loss: 0.2992558806152428  params: [-1.86504106  4.2181313  -3.99629197]  gradients: [0.014337709115415247, 0.0037218869950502217, -0.0001771808123605112]\n",
      "epoch: 500  loss: 0.2991919177143735  params: [-1.87238676  4.24176798 -4.01801598]  gradients: [0.014439254869398028, 0.0037494090863584768, -0.0002718993890771341]\n",
      "epoch: 600  loss: 0.29916935152149776  params: [-1.87524786  4.25097202 -4.02647263]  gradients: [0.014478669871985688, 0.003760071108523579, -0.00030855141497514524]\n",
      "epoch: 700  loss: 0.2991608736207429  params: [-1.87636789  4.25457479 -4.02978244]  gradients: [0.014494078856198347, 0.0037642361581706886, -0.00032286320150516217]\n",
      "epoch: 800  loss: 0.2991576028048108  params: [-1.87680722  4.25598789 -4.03108058]  gradients: [0.014500119711306377, 0.00376586851110296, -0.00032847131330620275]\n",
      "epoch: 900  loss: 0.29915632726033936  params: [-1.87697967  4.25654259 -4.03159015]  gradients: [0.014502490523814176, 0.003766509073677012, -0.0003306718898336978]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.87704698,  4.25675908, -4.03178901])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train,learning_rate=0.01, batch_size= 64) # MGD\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6860786534744289  params: [-0.10968034  0.43634108  0.22625849]  gradients: [0.40872497891892373, 0.15228153948293843, 0.472945590311182]\n",
      "epoch: 100  loss: 0.3023798457213181  params: [-1.62850332  3.48957394 -3.31488439]  gradients: [0.0027449672476518626, -0.00903102987870028, 0.008440562271695524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.78082106,  3.98736604, -3.77765723])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.01, batch_size = len(X_train)) # BGD\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0383637324127009  params: [0.29880605 0.1816933  0.28947564]  gradients: [0.00648618940865367, 0.0035297904800225084, 0.0045636228174223414]\n",
      "epoch: 100  loss: 0.11941529659046486  params: [-1.62680404  3.46015266 -3.29887879]  gradients: [0.0011276274381459331, 0.0006136559303786764, 0.0007933882256673788]\n",
      "epoch: 200  loss: 0.09717816921291196  params: [-1.78722101  3.97905939 -3.78119467]  gradients: [0.0009274536712645416, 0.0005047211749820031, 0.0006525478165404864]\n",
      "epoch: 300  loss: 0.09081112649416749  params: [-1.84027698  4.14954254 -3.93855716]  gradients: [0.0008693334837082603, 0.0004730921133237701, 0.0006116549906648131]\n",
      "epoch: 400  loss: 0.08856147538606417  params: [-1.85995412  4.21265672 -3.99668208]  gradients: [0.0008487115319765412, 0.0004618696274671299, 0.000597145576348744]\n",
      "epoch: 500  loss: 0.08771125282635572  params: [-1.86752654  4.23692977 -4.01901766]  gradients: [0.0008409059767781415, 0.00045762183686239634, 0.0005916536599766225]\n",
      "epoch: 600  loss: 0.08738190068714866  params: [-1.87048042  4.24639603 -4.02772552]  gradients: [0.0008378805859101891, 0.0004559754162583668, 0.0005895250229716365]\n",
      "epoch: 700  loss: 0.08725310830171198  params: [-1.87163867  4.25010754 -4.03113926]  gradients: [0.0008366972495194336, 0.00045533144346269415, 0.0005886924384426781]\n",
      "epoch: 800  loss: 0.08720255879561054  params: [-1.87209376  4.25156577 -4.03248043]  gradients: [0.0008362327631944588, 0.0004550786695603765, 0.0005883656301647381]\n",
      "epoch: 900  loss: 0.08718269009803543  params: [-1.87227271  4.25213917 -4.03300779]  gradients: [0.000836050188606689, 0.00045497931229508546, 0.0005882371723750971]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.87234267,  4.25236334, -4.03321395])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.01, batch_size = 1) # sgd\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 세 결과는 optimizer 방법에 따른 성능을 비교한 것이다. \n",
    "    \n",
    "    MGD의 경우 loss가 천천히 떨어짐을 확인 할 수 있고, parameters또한 값이 조금식 변한다는 것을 확인할 수 있다.\n",
    "    이에 비해 BGD의 경우 100번의 epoch 끝에 학습이 종료 되었는데 이는 tolerance, step이 너무 작아 더이상 학습이 무의미 하다는것을 알 수 있다. 이것을 다르게 말하면 Local minimum의 문제를 발생했다고 볼 수 있다.\n",
    "    SGD의 경우 앞으 두 경우와 다르게 굉장히 낮은 loss를 볼 수 있다. 100 epochs 사이에 loss값이 1에서 0.1로 줄었다. 이 경우는 앞의 BGD의 경우와 다르게 Global minimum에 도착했다고 볼 수 있다. \n",
    "    \n",
    "    세 경우 파라미터를 비교해 보면 큰 차이는 나지 않지만 loss차이가 큰 것을 보아 하나의 최적화 방법만 고집할 것이 아니라 다양한 최적하 방법을 고려 해야겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 완성한 함수들을 통해 test에 적용시켜 y_hat을 구한 후 모델을 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.94\n",
      "precision: 0.8181818181818182\n",
      "recall: 0.9\n",
      "f1_score: 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"accuracy:\",accuracy)\n",
    "print(\"precision:\",precision)\n",
    "print(\"recall:\",recall)\n",
    "print(\"f1_score:\",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    성능을 확인해 보면 정확도 94%, precision 81%, recall 90%, f1_score = 85%로 모든 성능지표를 고려하여도 좋은 모델을 만듦을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61376898, 2.67551012])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: -1.5258474839719138  params: [0.59733168 0.51363395]  gradients: [-0.0002183168996972886, -0.00015773689055796516]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.20752783, 1.61390649])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate= 0.0001,num_epoch=100 ,batch_size = 1)\n",
    "new_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임의로 생성한 데이터로 인해 loss는 음수가 나오며 epochs를 늘릴수록 loss는 nan값에 가까워져 parameters들이 기하 급수적으로 증가하여 제한된 모델을 설정하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 결과는 임의의 데이터를 설명하는 정규방정식과 경사하강법으로 구한 parameter들로 plot을 그려본 결과이다. 두 함수의 경우 차이가 나지만 어느정도 데이터를 설명할 수 있음을 확인 할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
